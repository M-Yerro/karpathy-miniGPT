{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Atención desnuda.\n",
        "\n",
        "El mecanismo de la atención paso a paso\n",
        "\n",
        "```\n",
        "# Tiene formato de código\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Copia de: https://github.com/jostmey/NakedAttention/tree/8a808e1344989a00082b33b3a7ab38410b599747"
      ],
      "metadata": {
        "id": "PlmTFPTVsCft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jrWTSTZtRMi",
        "outputId": "edc2ba11-7299-4e4a-9de0-60dc0b3eb037"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u6kl2i0Dr8pM"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bloque de dataplumbing:"
      ],
      "metadata": {
        "id": "Im9nqNPYsm7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##########################################################################################\n",
        "# Carga datos del MNIST\n",
        "##########################################################################################\n",
        "\n",
        "# Load training, validation, and test data from the MNIST dataset\n",
        "#\n",
        "def load_mnist(seed=None, device=torch.device('cpu')):\n",
        "\n",
        "  # Random number generator\n",
        "  # \n",
        "  generator = torch.Generator(device=device)\n",
        "  if seed is not None:\n",
        "    generator.manual_seed(seed)\n",
        "\n",
        "  # Load MNIST dataset\n",
        "  #\n",
        "  samples_train = torchvision.datasets.MNIST('./', train=True, download=True)\n",
        "  samples_test = torchvision.datasets.MNIST('./', train=False, download=True)\n",
        "\n",
        "  # Format features and labels\n",
        "  #\n",
        "  xs = samples_train.data.to(device)\n",
        "  num = xs.shape[0]\n",
        "  xs = xs.reshape([ num, 28**2, 1 ])\n",
        "  xs = xs.type(torch.float32)\n",
        "  ys = samples_train.train_labels.to(device)\n",
        "\n",
        "  xs_test = samples_test.data.to(device)\n",
        "  num_test = xs_test.shape[0]\n",
        "  xs_test = xs_test.reshape([ num_test, 28**2, 1 ])\n",
        "  xs_test = xs_test.type(torch.float32)\n",
        "  ys_test = samples_test.test_labels.to(device)\n",
        "\n",
        "  # Split into training and validation samples\n",
        "  #\n",
        "  num_train = int(num*5/6)\n",
        "  num_val = num-num_train\n",
        "\n",
        "  js = torch.randperm(num, generator=generator)\n",
        "  js_train = js[:num_train]\n",
        "  js_val = js[num_train:]\n",
        "\n",
        "  xs_train = xs[js_train]\n",
        "  ys_train = ys[js_train]\n",
        "\n",
        "  xs_val = xs[js_val]\n",
        "  ys_val = ys[js_val]\n",
        "\n",
        "  # Normalizing features\n",
        "  #\n",
        "  mean = torch.mean(xs_train, axis=0, keepdim=True)\n",
        "  variance = torch.var(xs_train, axis=0, keepdim=True)\n",
        "\n",
        "  xs_train = (xs_train-mean)/torch.std(variance+1.0E-8)\n",
        "  xs_val = (xs_val-mean)/torch.std(variance+1.0E-8)\n",
        "  xs_test = (xs_test-mean)/torch.std(variance+1.0E-8)\n",
        "\n",
        "  return xs_train, ys_train, xs_val, ys_val, xs_test, ys_test"
      ],
      "metadata": {
        "id": "TKFU5FuWsose"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################################################\n",
        "# Model\n",
        "##########################################################################################\n",
        "\n",
        "class SelfAttentionModel(torch.nn.Module):\n",
        "  def __init__(self, num_steps, num_channels, num_outputs, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    # Initialize components for self-attention\n",
        "    #\n",
        "    self.K = torch.nn.Parameter((2.0*torch.rand(num_channels, num_channels)-1.0)/num_channels**0.5) # Randomly intialize each weight uniformly from [ -1/num_channels**0.5, 1/num_channels**0.5 ]\n",
        "    self.Q = torch.nn.Parameter((2.0*torch.rand(num_channels, num_channels)-1.0)/num_channels**0.5)\n",
        "    self.V = torch.nn.Parameter((2.0*torch.rand(num_channels, num_channels)-1.0)/num_channels**0.5)\n",
        "\n",
        "    self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    # Initialize output layer\n",
        "    #\n",
        "    self.out = torch.nn.Linear(num_steps*num_channels, num_outputs)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    batch_size, num_steps, num_channels = x.shape\n",
        "\n",
        "    # Run self attention\n",
        "    #\n",
        "    y = []\n",
        "    for i in range(batch_size): # Process one sample at a time\n",
        "\n",
        "      x_i = x[i,:,:] # x_i has shape of [ num_steps, num_channels ]\n",
        "\n",
        "      x_k_i = torch.matmul(x_i, self.K) # x_k_i has shape of [ num_steps, num_channels ]\n",
        "      x_q_i = torch.matmul(x_i, self.Q) # x_q_i has shape of [ num_steps, num_channels ]\n",
        "      x_v_i = torch.matmul(x_i, self.V) # x_v_i has shape of [ num_steps, num_channels ]\n",
        "\n",
        "      w_i = self.softmax(torch.matmul(x_q_i, x_k_i.T)/num_channels**0.5) # w_i has shape of [ num_steps, num_steps ]\n",
        "      y_i = torch.matmul(w_i, x_v_i) # y_i has shape of [ num_steps, num_channels ]\n",
        "\n",
        "      y.append(y_i)\n",
        "    y = torch.stack(y, axis=0) # y has shape of [ batch_size, num_steps, num_channels ]\n",
        "\n",
        "    # Flatten output\n",
        "    #\n",
        "    y_flat = y.reshape([ batch_size, num_steps*num_channels ]) # y_flat has shape of [ batch_size, num_steps*num_channels ]\n",
        "\n",
        "    # Run output layer\n",
        "    #\n",
        "    l = self.out(y_flat) # l has shape of [ batch_size, num_outputs ]\n",
        "\n",
        "    return l\n",
        "\n",
        "##########################################################################################\n",
        "# Instantiate model, performance metrics, and optimizer.\n",
        "##########################################################################################\n",
        "\n",
        "model = SelfAttentionModel(num_steps=28**2, num_channels=1, num_outputs=10)\n",
        "probability = torch.nn.Softmax(dim=1)\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "accuracy = torchmetrics.classification.MulticlassAccuracy(num_classes=10)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "##########################################################################################\n",
        "# Dataset and data sampler\n",
        "##########################################################################################\n",
        "\n",
        "xs_train, ys_train, xs_val, ys_val, xs_test, ys_test = load_mnist(seed=46525)\n",
        "\n",
        "dataset_train = torch.utils.data.TensorDataset(xs_train, ys_train)\n",
        "sampler_train = torch.utils.data.RandomSampler(dataset_train, replacement=True)\n",
        "loader_train = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=16, sampler=sampler_train, drop_last=True)\n",
        "\n",
        "##########################################################################################\n",
        "# Model\n",
        "##########################################################################################\n",
        "\n",
        "i_better = -1\n",
        "e_better = 1.0e8\n",
        "a_better = 0.0\n",
        "state_better = {}\n",
        "\n",
        "# Loop over the dataset for many epochs\n",
        "#\n",
        "for i in range(128):\n",
        "\n",
        "  # Train the model\n",
        "  #\n",
        "  model.train()\n",
        "  e_train = 0.0\n",
        "  a_train = 0.0\n",
        "  for xs_batch, ys_batch in iter(loader_train): # Must use `iter` or `enumerate` for efficiency\n",
        "    ls_batch = model(xs_batch)\n",
        "    ps_batch = probability(ls_batch) # Model outputs logits that we must convert to probabilities\n",
        "    e_batch = loss(ls_batch, ys_batch) # CrossEntropyLoss requires logits\n",
        "    a_batch = accuracy(ps_batch, ys_batch)\n",
        "    optimizer.zero_grad()\n",
        "    e_batch.backward()\n",
        "    optimizer.step()\n",
        "    e_train += e_batch.detach()/len(loader_train) # Accumulate average loss for this epoch\n",
        "    a_train += a_batch.detach()/len(loader_train) # Accumulate average accuracy for this epoch\n",
        "\n",
        "  # Assess performance on validation data\n",
        "  #\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    ls_val = model(xs_val)\n",
        "    ps_val = probability(ls_val) # Model outputs logits that we must convert to probabilities\n",
        "    e_val = loss(ls_val, ys_val) # CrossEntropyLoss requires logits\n",
        "    a_val = accuracy(ps_val, ys_val)\n",
        "    if e_val < e_better: # Early stopping check\n",
        "      i_better = i\n",
        "      e_better = e_val\n",
        "      a_better = a_val\n",
        "      state_better = model.state_dict()\n",
        "\n",
        "  # Print report\n",
        "  #\n",
        "  print(\n",
        "    'i: '+str(i),\n",
        "    'e_train: {:.5f}'.format(float(e_train)/0.693)+' bits',\n",
        "    'a_train: {:.1f}'.format(100.0*float(a_train))+' %',\n",
        "    'e_val: {:.5f}'.format(float(e_val)/0.693)+' bits',\n",
        "    'a_val: {:.1f}'.format(100.0*float(a_val))+' %',\n",
        "    sep='\\t', flush=True\n",
        "  )\n",
        "\n",
        "model.eval()\n",
        "model.load_state_dict(state_better)\n",
        "with torch.no_grad():\n",
        "  ls_test = model(xs_test)\n",
        "  ps_test = probability(ls_test) # Model outputs logits that we must convert to probabilities\n",
        "  e_test = loss(ls_test, ys_test) # CrossEntropyLoss requires logits\n",
        "  a_test = accuracy(ps_test, ys_test)\n",
        "\n",
        "print(\n",
        "  'e_test: {:.5f}'.format(float(e_test)/0.693)+' bits',\n",
        "  'a_test: {:.1f}'.format(100.0*float(a_test))+' %',\n",
        "  sep='\\t', flush=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSy_OrCus7sk",
        "outputId": "9bb9914f-944c-4f28-fe17-5341bef629d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 102760359.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 17209219.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 26323389.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 13039376.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i: 0\te_train: 1.89826 bits\ta_train: 44.2 %\te_val: 0.56164 bits\ta_val: 88.9 %\n",
            "i: 1\te_train: 0.51991 bits\ta_train: 73.6 %\te_val: 0.52756 bits\ta_val: 89.5 %\n",
            "i: 2\te_train: 0.47493 bits\ta_train: 74.2 %\te_val: 0.49232 bits\ta_val: 90.5 %\n",
            "i: 3\te_train: 0.44804 bits\ta_train: 74.9 %\te_val: 0.46489 bits\ta_val: 91.3 %\n",
            "i: 4\te_train: 0.43954 bits\ta_train: 74.9 %\te_val: 0.46276 bits\ta_val: 91.6 %\n",
            "i: 5\te_train: 0.42702 bits\ta_train: 75.1 %\te_val: 0.46872 bits\ta_val: 91.2 %\n"
          ]
        }
      ]
    }
  ]
}